{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objective</a></span></li><li><span><a href=\"#Linear-Discriminant-Analysis\" data-toc-modified-id=\"Linear-Discriminant-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><a href=\"https://en.wikipedia.org/wiki/Linear_discriminant_analysis\" rel=\"nofollow\" target=\"_blank\">Linear Discriminant Analysis</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Theory-and-Model\" data-toc-modified-id=\"Theory-and-Model-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Theory and Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Head-the-Problem\" data-toc-modified-id=\"Head-the-Problem-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Head the Problem</a></span></li><li><span><a href=\"#Transform-the-Problem\" data-toc-modified-id=\"Transform-the-Problem-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Transform the Problem</a></span></li><li><span><a href=\"#Solve-the-Problem\" data-toc-modified-id=\"Solve-the-Problem-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Solve the Problem</a></span></li></ul></li><li><span><a href=\"#MultiClasses-Problem\" data-toc-modified-id=\"MultiClasses-Problem-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>MultiClasses Problem</a></span><ul class=\"toc-item\"><li><span><a href=\"#Derivation\" data-toc-modified-id=\"Derivation-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Derivation</a></span></li></ul></li><li><span><a href=\"#Summary-for-LDA\" data-toc-modified-id=\"Summary-for-LDA-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Summary for LDA</a></span></li></ul></li><li><span><a href=\"#Code-Implementation\" data-toc-modified-id=\"Code-Implementation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Code Implementation</a></span></li><li><span><a href=\"#LDA-With-scikit-learn\" data-toc-modified-id=\"LDA-With-scikit-learn-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>LDA With scikit-learn</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Tune-LDA-Hyperparameters\" data-toc-modified-id=\"Tune-LDA-Hyperparameters-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Tune LDA Hyperparameters</a></span></li></ul></li></ul></li><li><span><a href=\"#LAB-Assignment\" data-toc-modified-id=\"LAB-Assignment-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>LAB Assignment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise-1-Linear-Discriminant-Analysis-from-Scratch-using-numpy-(50-points-)\" data-toc-modified-id=\"Exercise-1-Linear-Discriminant-Analysis-from-Scratch-using-numpy-(50-points-)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Exercise 1 Linear Discriminant Analysis from Scratch using numpy (50 points )</a></span></li><li><span><a href=\"#Exercise-2-Recognize-handwritten-numbers-with-LDA-(50-points-).\" data-toc-modified-id=\"Exercise-2-Recognize-handwritten-numbers-with-LDA-(50-points-).-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Exercise 2 Recognize handwritten numbers with LDA (50 points ).</a></span></li><li><span><a href=\"#MNIST-Dataset\" data-toc-modified-id=\"MNIST-Dataset-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>MNIST Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#MNIST-Dataset-File-Formats\" data-toc-modified-id=\"MNIST-Dataset-File-Formats-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>MNIST Dataset File Formats</a></span></li></ul></li></ul></li><li><span><a href=\"#THE-IDX-FILE-FORMAT\" data-toc-modified-id=\"THE-IDX-FILE-FORMAT-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>THE IDX FILE FORMAT</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB6 tutorial for Machine Learning <br >Linear Discriminant Analysis(LDA)\n",
    "> The document description are designed by JIa Yanhong in 2022. Oct. 3th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Learn LDA’s theoretical concepts.  \n",
    "- Implement LDA from scratch using NumPy.\n",
    "- How to fit, evaluate, and make predictions with the Linear Discriminant Analysis model with Scikit-Learn.\n",
    "- How to tune the hyperparameters of the Linear Discriminant Analysis algorithm on a given dataset.\n",
    "- Complete the LAB assignment.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Linear Discriminant Analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)\n",
    "\n",
    "Linear Discriminant Analysis(LDA) is a supervised learning algorithm used as a classifier and a dimensionality reduction algorithm. The basic idea of LDA is to project data into a low-dimensional space, so that **the same class of data is as close as possible, while different classes of data are as far away as possible**. \n",
    "\n",
    "![](images/example.png)\n",
    "\n",
    "The left plot shows samples from two classes (depicted in red and blue) along with the histograms resulting from projection onto the line joining the class means. Note\n",
    "that there is considerable class overlap in the projected space. \n",
    "\n",
    "The right plot shows the corresponding projection based on the Fisher linear discriminant, showing the\n",
    "greatly improved class separation.\n",
    "\n",
    "So our job is seeking to obtain a scalar  $y$ by projecting the samples $X$ onto a line:\n",
    "$$\n",
    "y = \\theta^{T}X\n",
    "$$ \n",
    "\n",
    "Then try to find the $\\theta^\\ast$ to maximize the ratio of `between-class variance` to `within-class variance`. Next, we will introduce how to use mathematic way to present this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory and Model\n",
    "To figure out the LDA, first we need know how to translate `between-class variance` and `within-class variance` to mathematic language. Then we try to maximize the ratio between these two. To simplify the problem, we start with two classes problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Head the Problem\n",
    "Assume we have a set of D-dimensional samples $X = \\{x^{(1)}, x^{(2)}, ... x^{(m)} \\}$, $N_1$ of which belong to class $C_1$, and $N_2$ of which belong to class $C_2$.\n",
    "We also assume the mean vector of two classes in X-space:\n",
    "$$\n",
    "\tu_k = \\frac{1}{N_k} \\sum_{i \\in C_k} x ^{(i)} \\quad \\textrm{where}  \\quad k = 1, 2.\n",
    "$$\n",
    "and in y-space:\n",
    "$$\n",
    "\t{\\hat{u}}_k = \\frac{1}{N_k} \\sum_{i \\in C_k} y^{(i)} = \\frac{1}{N_k} \\sum_{i \\in C_k} \\theta^{T}x^{(i)} = \\theta^{T}u_k \\quad \\textrm{where} \\quad k = 1, 2.\n",
    "$$\n",
    "\n",
    "One way to define a measure of separation between two classes is to choose the distance between the projected means, which is in y-space, so the `between-class variance` is:\n",
    "$$\n",
    "\t\\hat{u}_2 - \\hat{u}_1 =  \\theta^{T}(u_2 - u_1)\n",
    "$$\n",
    "Also, we can define the `within-class variance` for each class $C_k$ is:\n",
    "$$\n",
    "\t\\hat{s}^{2}_k = \\sum_{i \\in C_k} (y^{(i)}-\\hat{u}_k)^2 \\quad \\textrm{where} \\quad k = 1,2.\n",
    "$$\n",
    "Then, we get the between-class variance and within-class variance, we can define our objective function $J(\\theta)$ as:\n",
    "$$\n",
    "\tJ(\\theta) = \\frac{(\\hat{u}_2 -\\hat{u}_1)^2} {\\hat{s}^{2}_1 + \\hat{s}^{2}_2}\n",
    "$$\n",
    "In fact, if maximizing the objective function $J$, we are looking for a projection where examples from the class are projected very close to each other and at the same time, the projected means are as farther apart as possible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the Problem\n",
    "To find the optimum $\\theta^\\ast$, we must express $J(\\theta)$ as a function of $\\theta$. Before the optimum,we need introduce **scatter** instead of variance. \n",
    "\n",
    "We define some measures of the scatter as following:\n",
    "\n",
    "- The scatter in feature space-x: $  S_k = \\sum_{i \\in C_k} (x^{(i)} - u_k) (x^{(i)} - u_k)^{T} $ \n",
    "- Within-class scatter matrix: $S_W = S_1 + S_2$\n",
    "- Between-class scather matrix: $S_B = (u_2 - u_1)(u_2 - u_1)^T$\n",
    "\n",
    "Let's see $J(\\theta)$ again:\n",
    "$$\n",
    "\tJ(\\theta) = \\frac{(\\hat{u}_2 -\\hat{u}_1)^2} {\\hat{s}^{2}_1 + \\hat{s}^{2}_2}\n",
    "$$\n",
    "The scatter of the projection $y$ can then be expressed as a function of the scatter matrix in feature space $x$:\n",
    "\\begin{align*}\n",
    "\t\\hat{s}^{2}_k &= \\sum_{i \\in C_k} (y^{(i)}-\\hat{u}_k)^2 \\\\\n",
    "\t&= \\sum_{i \\in C_k}(\\theta^{T}x^{(i)} - \\theta^{T}u_k )^2  \\\\\n",
    "\t&= \\sum_{i \\in C_k} \\theta^{T}(x^{(i)} - u_k)(x^{(i)} - u_k)^T\\theta \\\\\n",
    "\t&= \\theta^T S_k \\theta\n",
    "\\end{align*}\n",
    "So we can get:\n",
    "\\begin{align*}\n",
    "\t\\hat{s}^{2}_1 + \\hat{s}^{2}_2 &= \\theta^T S_1 \\theta + \\theta^T S_2 \\theta \\\\\n",
    "\t&= \\theta^T S_W \\theta\n",
    "\\end{align*}\n",
    "Similarly, the difference between the projected means can be expressed in terms \n",
    "of the means in the original feature space:\n",
    "\\begin{align*}\n",
    "\t(\\hat{u}_2 -\\hat{u}_1)^2 &= (\\theta^T u_2 - \\theta^T u_1)^2 \\\\\n",
    "\t&= \\theta^T (u_2 - u_1)(u_2 - u_1)^T \\theta \\\\\n",
    "\t&= \\theta^TS_B\\theta\n",
    "\\end{align*}\n",
    "We can finally express the Fisher criterion in terms of $S_W$ and $S_B$ as:\n",
    "$$\n",
    "\tJ(\\theta) = \\frac{\\theta^T S_B \\theta} {\\theta^T S_W \\theta}\n",
    "$$\n",
    "Next, we will maximize this objective function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve the Problem\n",
    "The easiest way to maximize the object function $J$ is to derive it and set it to zero.\n",
    "\\begin{align*}\n",
    "\t\\frac {\\partial J(\\theta)}{\\partial \\theta} &= \\frac {\\partial } {\\partial \\theta} (\\frac{\\theta^T S_B \\theta} {\\theta^T S_W \\theta}) \\\\\n",
    "\t&= (\\theta^T S_W \\theta) \\frac{\\partial (\\theta^T S_B \\theta)} {\\partial \\theta} - (\\theta^T S_B \\theta) \\frac {\\partial (\\theta^T S_W \\theta)} {\\partial \\theta} = 0 \\\\\n",
    "\t\\implies &= (\\theta^T S_W \\theta) 2 S_B \\theta - (\\theta^T S_B \\theta) 2 S_W \\theta = 0\n",
    "\\end{align*}\n",
    "Divided by $\\theta^T S_W \\theta:$ \n",
    "\\begin{align*}\n",
    "\t&\\implies (\\frac{\\theta^T S_W \\theta} {\\theta^T S_W \\theta})S_B\\theta - (\\frac{\\theta^T S_B \\theta} {\\theta^T S_W \\theta})S_W \\theta = 0 \\\\\n",
    "\t&\\implies S_B \\theta - J S_W \\theta = 0 \\\\\n",
    "\t&\\implies S^{-1}_W S_B \\theta - J\\theta = 0 \\\\\n",
    "\t&\\implies  J\\theta = S^{-1}_W S_B \\theta  \\\\\n",
    "\t&\\implies  J\\theta = S^{-1}_W (u_2 - u_1)(u_2 - u_1)^T \\theta  \\\\\n",
    "\t&\\implies  J\\theta = S^{-1}_W (u_2 - u_1) (\\underbrace{(u_2 - u_1)^T \\theta}_{c \\in \\mathbb{R}})  \\\\\n",
    "\t&\\implies  J\\theta = c S^{-1}_W(u_2 - u_1)   \\\\\n",
    "\t&\\implies \\theta = \\frac{c}{J} S^{-1}_W(u_2 - u_1)\n",
    "\\end{align*}\n",
    "For now, the problem has been solved and we just want to get the direction of the $\\theta$, which is the optimum $ \\theta^\\ast$:\n",
    "$$\n",
    "\t\\theta^{\\ast}  \\propto S^{-1}_W(u_2 - u_1)\n",
    "$$\n",
    "This is known as Fisher's linear discriminant(1936), although it is not a discriminant but rather a specific choice of direction for the projection of the data down to one dimension, which is $y = \\theta^{\\ast T}X$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiClasses Problem\n",
    "Based on two classes problem, we can see that the fisher's LDA generalizes gracefully for multiple classes problem. Assume we still have a set of D-dimensional samples $X = \\{x^{(1)}, x^{(2)}, ... , x^{(m)} \\}$, and there are totally $C$ classes. Instead of one projection $y$, mentioned above, we now will seek $(C-1)$ projections $[y_1, y_2, \\dots y_{C-1}]$ by means of $(C-1)$ projection vectors $\\theta_i$ arranged by columns into a projection matrix $\\Theta = [\\theta_1 | \\theta_2 | \\dots | \\theta_{C-1}]$, where: \n",
    "$$\n",
    "\ty_i  = \\theta_i^{T}X \n",
    "\t\\implies y = \\Theta^T X\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation\n",
    "First we will use the scatters in space-x as following:\n",
    "\n",
    "- Within-class scatter matrix:\n",
    "$$\n",
    "S_W = \\sum_{i=1}^{C}S_i \\quad \\text{where} \\quad S_i = \\sum_{i \\in C_i}(x^{(i) }- u_i)(x^{(i)} - u_i)^T \\: \\text{and} \\: u_i = \\frac{1}{N_i} \\sum_{i \\in C_i} x^{(i)}\n",
    "$$\n",
    "- Between-class scatter matrix:\n",
    "$$\n",
    "S_B = \\sum_{i=1}^{C}N_i (u_i - u)(u_i - u)^T \\quad \\text{where} \\quad u = \\frac{1}{m}\\sum_{i=1}^{m}x^{(i)} = \\frac{1}{m}\\sum_{i=1}^{C}N_i u_i\n",
    "$$\n",
    "- Total scatter matrix:\n",
    "$$\n",
    "S_T = S_B + S_W\n",
    "$$\n",
    "\n",
    "\n",
    "Before moving on, let us see a picture for the multi-class example:\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src='images/multi-16656247203831.png' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "      LDA Multi-Class examples\n",
    "  \t</div>\n",
    "</center><br>\n",
    "\n",
    "Similarly, we define the mean vector and scatter matrices for the projected samples as:\n",
    "\n",
    "- $\\hat{u}_i = \\frac{1}{N_i} \\sum_{i \\in C_i} y^{(i)}$ \n",
    "- $\\hat{u} = \\frac{1}{N} \\sum_{i=1}^{m}y^{(i)}$\n",
    "- $ \\hat{S}_W = \\sum_{i=1}^{C}\\sum_{y \\in C_i} (y - \\hat{u}_i)(y - \\hat{u}_i)^T $\n",
    "- $\\hat{S}_B = \\sum_{i=1}^{C} N_i (\\hat{u}_i - \\hat{u})(\\hat{u}_i - \\hat{u})^T$\n",
    "\n",
    "From our derivation for the two-class problem, we can get:\n",
    "\\begin{align}\n",
    "\t\\hat{S}_W = \\Theta^T S_W \\Theta \\\\\n",
    "\t\\hat{S}_B =  \\Theta^T S_B \\Theta\n",
    "\\end{align}\n",
    "Recall that we are looking for a projection that maximizes the ratio of between-class to within-class scatter. Since the projection is no longer a scalar (it has $C-1$ dimensions), we use the trace of the scatter matrices to obtain a scalar objective function:\n",
    "$$\n",
    "\tJ(W) = \\frac{trace(\\hat{S}_B)}{trace(\\hat{S}_W)} = \\frac{trace(\\Theta^T S_B \\Theta)}{trace(\\Theta^T S_W \\Theta)}\n",
    "$$\n",
    "And now, our job is to seek the projection matrix $\\Theta^{\\ast} $that maximize this ratio. We will not give the derivation process. But we know that the optimal projection matrix $\\Theta^{\\ast}$ is the one whose columns are the eigenvectors corresponding to the largest eigenvalues of the following generalized eigenvalue problem:\n",
    "\\begin{align*}\n",
    "\t\\Theta^{\\ast} &= [\\theta_{1}^{\\ast} | \\theta_{2}^{\\ast} | \\dots | \\theta_{C-1}^{\\ast}] \\\\\n",
    "\t&= \\text{argmax} \\frac{trace(\\Theta^T S_B \\Theta)}{trace(\\Theta^T S_W \\Theta)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\t\\implies (S_B - \\lambda_i S_W)\\theta_{i}^{\\ast} = 0\n",
    "$$\n",
    "Thus, if $S_W$ is a non-singular matrix, and can be inverted, then the Fisher's criterion is maximized when the projection matrix $\\Theta^{\\ast}$ is composed of the eigenvectors of:\n",
    "$$\n",
    "\tS^{-1}_WS_B\n",
    "$$\n",
    "Noticed that, there will be at most $C-1$ eigenvectors with non-zero real corresponding eigenvalues $\\lambda_i$. This is because $S_B$ is of rank $(C-1)$ or less. So we can see that LDA can represent a massive reduction in the dimensionality of the problem. In face recognition for example there may be several thousand variables, but only a few hundred classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary for LDA\n",
    "In summary, LDA can be performed in 5 steps:\n",
    "\n",
    "1. Compute the mean $\\mu_i$ for each class and the global mean $\\mu$ for all samples.\n",
    "2. Compute  the within-class scatter matrix $S_w$, the global scatter matrix $S_t$, and the between-class scatter matrix $S_b$ .\n",
    "3. Compute the eigen vectors and corresponding eigen values for $S_w^{-1}S_b$ .\n",
    "4. Sort the eigenvectors by decreasing eigenvalues and choose $C-1$ eigenvectors with the largest eigenvalues.\n",
    "5. Form the projection matrix with chosen eigenvectors\n",
    "6. Use this eigenvector matrix to transform the samples onto the new subspace. $Y = X \\times W $\n",
    "   \n",
    " \n",
    "------  \n",
    "\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implementation \n",
    "**This is your lab assignment ！！！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------\n",
    "\n",
    "## LDA With scikit-learn\n",
    "The Linear Discriminant Analysis is available in the scikit-learn Python machine learning library via the [LinearDiscriminantAnalysis class](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html).\n",
    "\n",
    "The method can be used directly without configuration, although the implementation does offer arguments for customization, such as the choice of solver and the use of a penalty.\n",
    "\n",
    "\n",
    " ```python\n",
    "# create the lda model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "```\n",
    "\n",
    "We can demonstrate the Linear Discriminant Analysis method with a worked example.\n",
    "\n",
    "First, let’s define a synthetic classification dataset.\n",
    "\n",
    "We will use the [make_classification() function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) to create a dataset with 1,000 examples, each with 10 input variables.\n",
    "\n",
    "The example creates and summarizes the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# test classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit and evaluate a Linear Discriminant Analysis model using [repeated stratified k-fold cross-validation](https://machinelearningmastery.com/k-fold-cross-validation/) via the [RepeatedStratifiedKFold class](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html). We will use 10 folds and three repeats in the test harness.\n",
    "\n",
    "The complete example of evaluating the Linear Discriminant Analysis model for the synthetic binary classification task is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.893 (0.033)\n"
     ]
    }
   ],
   "source": [
    "# evaluate a lda model on the dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n",
    "# define model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# summarize result\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example evaluates the Linear Discriminant Analysis algorithm on the synthetic dataset and reports the average accuracy across the three repeats of 10-fold cross-validation.\n",
    "\n",
    "Your specific results may vary given the stochastic nature of the learning algorithm. Consider running the example a few times.\n",
    "\n",
    "In this case, we can see that the model achieved a mean accuracy of about 89.3 percent.\n",
    "\n",
    "We may decide to use the Linear Discriminant Analysis as our final model and make predictions on new data.\n",
    "\n",
    "This can be achieved by fitting the model on all available data and calling the predict() function passing in a new row of data.\n",
    "\n",
    "We can demonstrate this with a complete example listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1608/193777306.py:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print('Predicted Class: %d' % yhat)\n"
     ]
    }
   ],
   "source": [
    "# make a prediction with a lda model on the dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n",
    "\n",
    "# define model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "# define new data\n",
    "row = [0.12777556,-3.64400522,-2.23268854,-1.82114386,1.75466361,0.1243966,1.03397657,2.35822076,1.01001752,0.56768485]\n",
    "# make a prediction\n",
    "yhat = model.predict([row])\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d' % yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can look at configuring the model hyperparameters.\n",
    "#### Tune LDA Hyperparameters\n",
    "\n",
    "The hyperparameters for the Linear Discriminant Analysis method must be configured for your specific dataset.\n",
    "\n",
    "An important hyperparameter is the solver, which defaults to ‘*svd*‘ but can also be set to other values for solvers that support the shrinkage capability.\n",
    "\n",
    "The example below demonstrates this using the [GridSearchCV class](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) with a grid of different solver values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.893\n",
      "Config: {'solver': 'svd'}\n"
     ]
    }
   ],
   "source": [
    "# grid search solver for lda\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n",
    "# define model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['solver'] = ['svd', 'lsqr', 'eigen']\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X, y)\n",
    "# summarize\n",
    "print('Mean Accuracy: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example will evaluate each combination of configurations using repeated cross-validation.\n",
    "\n",
    "Your specific results may vary given the stochastic nature of the learning algorithm. Try running the example a few times.\n",
    "\n",
    "In this case, we can see that the default SVD solver performs the best compared to the other built-in solvers.\n",
    "\n",
    "Next, we can explore whether using shrinkage with the model improves performance.\n",
    "\n",
    "Shrinkage adds a penalty to the model that acts as a type of regularizer, reducing the complexity of the model.\n",
    "\n",
    " > Regularization reduces the variance associated with the sample based estimate at the expense of potentially increased bias. This bias variance trade-off is generally regulated by one or more (degree-of-belief) parameters.\n",
    "\n",
    "This can be set via the “*shrinkage*” argument and can be set to a value between 0 and 1. We will test values on a grid with a spacing of 0.01.\n",
    "\n",
    "In order to use the penalty, a solver must be chosen that supports this capability, such as ‘*eigen*’ or ‘*lsqr*‘. We will use the latter in this case.\n",
    "\n",
    "The complete example of tuning the shrinkage hyperparameter is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.894\n",
      "Config: {'shrinkage': np.float64(0.02)}\n"
     ]
    }
   ],
   "source": [
    "# grid search shrinkage for lda\n",
    "from numpy import arange\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n",
    "# define model\n",
    "model = LinearDiscriminantAnalysis(solver='lsqr')\n",
    "# define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['shrinkage'] = arange(0, 1, 0.01)\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X, y)\n",
    "# summarize\n",
    "print('Mean Accuracy: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example will evaluate each combination of configurations using repeated cross-validation.\n",
    "\n",
    "Your specific results may vary given the stochastic nature of the learning algorithm. Try running the example a few times.\n",
    "\n",
    "In this case, we can see that using shrinkage offers a slight lift in performance from about 89.3 percent to about 89.4 percent, with a value of 0.02.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a prediction with a lda model on the dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20, shuffle=True)\n",
    "# define model\n",
    "model = LinearDiscriminantAnalysis(solver='lsqr',shrinkage=0.02)\n",
    "# fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make a prediction\n",
    "yhat = model.predict(X_test)\n",
    "# summarize prediction\n",
    "accuracy_score(y_test,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "------\n",
    "## LAB Assignment\n",
    "\n",
    "<font color=red>DDL: week 9.</font>\n",
    "\n",
    "### Exercise 1 Linear Discriminant Analysis from Scratch using numpy (50 points )\n",
    "- <font size=4> Complete the missing code in the LDA class below<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LDA:\n",
    "    #n_components:Number of components (<= min(n_classes - 1, n_features)) for dimensionality reduction.\n",
    "    def __init__(self, n_components=None):\n",
    "        \n",
    "        self.n_components = n_components\n",
    "        self.eigenvalues = None\n",
    "        self.eigenvectors = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.n_components is None or self.n_components > X.shape[1]:\n",
    "            n_components = X.shape[1]\n",
    "        else:\n",
    "            n_components = self.n_components\n",
    "       \n",
    "        n_features = np.shape(X)[1]\n",
    "        labels = np.unique(y)\n",
    "  \n",
    "        # Within class scatter matrix\n",
    "        # Complete code for calculating S_W\n",
    "        ########### Write Your Code Here ###########\n",
    "\n",
    "        # Between class scatter matrix\n",
    "        # Complete code for calculating S_W\n",
    "        ########### Write Your Code Here ###########\n",
    "      \n",
    "        # Determine SW^-1 * SB by calculating inverse of SW\n",
    "        ########### Write Your Code Here ###########\n",
    "\n",
    "        # Get eigenvalues and eigenvectors of SW^-1 * SB\n",
    "        ########### Write Your Code Here ###########\n",
    "\n",
    "        # Sort the eigenvalues and corresponding eigenvectors from largest\n",
    "        # to smallest eigenvalue and select the first n_components\n",
    "        idx = eigenvalues.argsort()[::-1]\n",
    "        eigenvalues = eigenvalues[idx][:n_components]\n",
    "        eigenvectors = eigenvectors[:, idx][:, :n_components]\n",
    "  \n",
    "        self.eigenvalues = eigenvalues\n",
    "        self.eigenvectors = eigenvectors\n",
    "  \n",
    "    def fit_transform(self, X):\n",
    "        ########### Write Your Code Here ###########\n",
    "        \n",
    "        return None  \n",
    "  \n",
    "    def transform(self, X):\n",
    "        ########### Write Your Code Here ###########\n",
    "        \n",
    "        return None  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font size=4> Dimensionality reduction visualization<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'eigenvalues' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m y \u001b[38;5;241m=\u001b[39m iris\u001b[38;5;241m.\u001b[39mtarget\n\u001b[1;32m     11\u001b[0m pca \u001b[38;5;241m=\u001b[39m LDA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m X \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m     15\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m, in \u001b[0;36mLDA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     20\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Within class scatter matrix\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Complete code for calculating S_W\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m########### Write Your Code Here ###########\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Sort the eigenvalues and corresponding eigenvectors from largest\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# to smallest eigenvalue and select the first n_components\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43meigenvalues\u001b[49m\u001b[38;5;241m.\u001b[39margsort()[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     39\u001b[0m eigenvalues \u001b[38;5;241m=\u001b[39m eigenvalues[idx][:n_components]\n\u001b[1;32m     40\u001b[0m eigenvectors \u001b[38;5;241m=\u001b[39m eigenvectors[:, idx][:, :n_components]\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'eigenvalues' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "pca = LDA(n_components=3)\n",
    "pca.fit(X, y)\n",
    "X = pca.transform(X)\n",
    "\n",
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "ax = fig.add_subplot(111, projection=\"3d\", elev=48, azim=134)\n",
    "ax.set_position([0, 0, 0.95, 1])\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:    \n",
    "    ax.text3D(\n",
    "        X[y == label, 0].mean(),\n",
    "        X[y == label, 1].mean(),\n",
    "        X[y == label, 2].mean() + 2,\n",
    "        name,\n",
    "        horizontalalignment=\"center\",\n",
    "        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n",
    "    )\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(float)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, \n",
    "        edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "ax.set_xlabel(\"Petal width\")\n",
    "ax.set_ylabel(\"Sepal length\")\n",
    "ax.set_zlabel(\"Petal length\")\n",
    "# ax.set_title(\"Ground Truth\")\n",
    "ax.dist = 12\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Recognize handwritten digits with LDA (50 points ). \n",
    "\n",
    "Your task in this section is to recognize handwritten numbers, and you can use the linear discriminant analysis model from the Scikit-Learn library to fit, evaluate, and predict them.\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "\n",
    "MNIST set is a large collection of **handwritten digits.** It is a very popular dataset in the field of image processing. It is often used for benchmarking machine learning algorithms. In simple terms, MNIST can be thought of as the “Hello, World!” of machine learning. MNIST is primarily used to experiment with different machine learning algorithms and to compare their relative strengths.\n",
    "\n",
    "MNIST contains a collection of **70,000, 28 x 28** images of handwritten digits from **0 to 9.**\n",
    "\n",
    "The dataset is already divided into training and testing sets. We will see this later in the tutorial.\n",
    "\n",
    "For more information on MNIST, refer to its [Wikipedia page](https://en.wikipedia.org/wiki/MNIST_database). We are going to import the dataset .\n",
    "\n",
    "Download and extract the MINIST dataset from the official website: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "![image-20221010151926993](images/image-20221010151926993.png)\n",
    "\n",
    "The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. \n",
    "Four files are available:\n",
    "\n",
    "- train-images-idx3-ubyte.gz: training set images (9912422 bytes)\n",
    "- train-labels-idx1-ubyte.gz: training set labels (28881 bytes)\n",
    "- t10k-images-idx3-ubyte.gz: test set images (1648877 bytes)\n",
    "- t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)\n",
    "\n",
    "#### MNIST Dataset File Formats\n",
    "\n",
    "The data is stored in a very simple file format designed for storing vectors and multidimensional matrices. General info on this format is given at the end of this page, but you don't need to read that to use the data files.\n",
    "\n",
    "All the integers in the files are stored in the MSB first (high endian) format used by most non-Intel processors. Users of Intel processors and other low-endian machines must flip the bytes of the header.\n",
    "\n",
    "There are 4 files:\n",
    "\n",
    "```\n",
    "train-images-idx3-ubyte: training set images\n",
    "train-labels-idx1-ubyte: training set labels \n",
    "t10k-images-idx3-ubyte: test set images \n",
    "t10k-labels-idx1-ubyte: test set labels\n",
    "```\n",
    "\n",
    "The training set contains 60000 examples, and the test set 10000 examples.\n",
    "\n",
    "The first 5000 examples of the test set are taken from the original NIST training set. The last 5000 are taken from the original NIST test set. The first 5000 are cleaner and easier than the last 5000.\n",
    "\n",
    "- TRAINING SET LABEL FILE (train-labels-idx1-ubyte):\n",
    "\n",
    "```\n",
    "[offset] [type]     [value]     [description] \n",
    "0000   32 bit integer 0x00000801(2049) magic number (MSB first) \n",
    "0004   32 bit integer 60000      number of items \n",
    "0008   unsigned byte  ??        label \n",
    "0009   unsigned byte  ??        label \n",
    "........\n",
    "xxxx   unsigned byte  ??        label\n",
    "The labels values are 0 to 9.\n",
    "```\n",
    "\n",
    "- TRAINING SET IMAGE FILE (train-images-idx3-ubyte):\n",
    "\n",
    "```\n",
    "[offset] [type]     [value]     [description] \n",
    "0000   32 bit integer 0x00000803(2051) magic number \n",
    "0004   32 bit integer 60000      number of images \n",
    "0008   32 bit integer 28        number of rows \n",
    "0012   32 bit integer 28        number of columns \n",
    "0016   unsigned byte  ??        pixel \n",
    "0017   unsigned byte  ??        pixel \n",
    "........ \n",
    "xxxx   unsigned byte  ??        pixel\n",
    "```\n",
    "\n",
    "Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).\n",
    "\n",
    "- TEST SET LABEL FILE (t10k-labels-idx1-ubyte):\n",
    "\n",
    "```\n",
    "[offset] [type]     [value]     [description]\n",
    "0000   32 bit integer 0x00000801(2049) magic number (MSB first)\n",
    "0004   32 bit integer 10000      number of items \n",
    "0008   unsigned byte  ??        label \n",
    "0009   unsigned byte  ??        label \n",
    "........ \n",
    "xxxx   unsigned byte  ??        label\n",
    "```\n",
    "\n",
    "The labels values are 0 to 9.\n",
    "\n",
    "- TEST SET IMAGE FILE (t10k-images-idx3-ubyte):\n",
    "\n",
    "```\n",
    "[offset] [type]     [value]     [description] \n",
    "0000   32 bit integer 0x00000803(2051) magic number \n",
    "0004   32 bit integer 10000      number of images \n",
    "0008   32 bit integer 28        number of rows \n",
    "0012   32 bit integer 28        number of columns \n",
    "0016   unsigned byte  ??        pixel \n",
    "0017   unsigned byte  ??        pixel \n",
    "........\n",
    "xxxx   unsigned byte  ??        pixel\n",
    "```\n",
    "\n",
    "Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).   \n",
    "\n",
    "\n",
    "## THE IDX FILE FORMAT\n",
    "\n",
    "the IDX file format is a simple format for vectors and multidimensional matrices of various numerical types.\n",
    "\n",
    "The basic format is\n",
    "\n",
    "```\n",
    "magic number\n",
    "size in dimension 0\n",
    "size in dimension 1\n",
    "size in dimension 2 \n",
    ".....\n",
    "size in dimension N \n",
    "data\n",
    "```\n",
    "\n",
    "The magic number is an integer (MSB first). The first 2 bytes are always 0.\n",
    "\n",
    "The third byte codes the type of the data: \n",
    "0x08: unsigned byte \n",
    "0x09: signed byte \n",
    "0x0B: short (2 bytes) \n",
    "0x0C: int (4 bytes) \n",
    "0x0D: float (4 bytes) \n",
    "0x0E: double (8 bytes)\n",
    "\n",
    "The 4-th byte codes the number of dimensions of the vector/matrix: 1 for vectors, 2 for matrices....\n",
    "\n",
    "The sizes in each dimension are 4-byte integers (MSB first, high endian, like in most non-Intel processors).\n",
    "\n",
    "The data is stored like in a C array, i.e. the index in the last dimension changes the fastest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3: Qustions\n",
    "- 1.What are the advantages and disadvantages of LDA?\n",
    "- 2.LDA can be used for dimensionality reduction, so can PCA. What are the specific similarities and differences between the two??\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
